{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95b01a1",
   "metadata": {},
   "source": [
    "### Provision Layer for American Express, credit account\n",
    "\n",
    "**Author**: Ricardo Pérez Castillo\n",
    "\n",
    "**Latest update**: 2024-12-30\n",
    "\n",
    "**Version**: 6.0\n",
    "\n",
    "**Purpose**: Prepare expense data into an unified single source of truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ef2ea",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Raw File Importing](#raw-file-importing)\n",
    "3. [Data Description](#data-description)\n",
    "4. [Basic Data Cleansing](#basic-data-cleansing)\n",
    "5. [Entity Harmonization](#entity-harmonization)\n",
    "6. [Transaction Type Definition](#transaction-type-definition)\n",
    "7. [Data Cleansing and Transformation](#data-cleansing-and-transformation)\n",
    "8. [Exporting](#exporting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c88c0ec",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "American Express is a credit provider. When extracting data from their portal, select the option that provides the most detail possible. Extract in excel format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c10d47",
   "metadata": {},
   "source": [
    "### Raw File Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf5248cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np  # For numerical computations\n",
    "from datetime import datetime  # To handle date and time operations\n",
    "import pathlib  # For handling and manipulating file paths in an object-oriented way\n",
    "from dateutil.relativedelta import relativedelta  # To handle date arithmetic\n",
    "\n",
    "# Import custom configurations and mappings\n",
    "from config import current_month, current_month_text, current_year  # Custom configurations for date handling\n",
    "from entity_mapping import entity_mapping  # Predefined mapping for harmonizing entities\n",
    "from transaction_subtype_mapping import transaction_subtype_mapping  # Predefined mapping for harmonizing transaction subtypes\n",
    "\n",
    "# Import utilities for fuzzy matching\n",
    "from difflib import get_close_matches  # To find close matches between strings\n",
    "\n",
    "# Import utilities for generating hash values\n",
    "import hashlib  # For generating hash values\n",
    "\n",
    "# Import utilities to remove spanish accents\n",
    "import unicodedata # For removing accents from strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97619ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory for file storage\n",
    "base_dir = pathlib.Path.home() / \"Documents\" / \"Finanzas\"\n",
    "# The pathlib.Path.home() dynamically retrieves the user's home directory.\n",
    "\n",
    "# Define the input directory for files\n",
    "input_dir = base_dir / \"AMEX\" / \"Movimientos\" / \"2024\"\n",
    "# This constructs the full path to the directory where input files are stored.\n",
    "# Example: \"C:/Users/YourUserName/Documents/Finanzas/BBVA/BBVA TDC/Movimientos/2024\"\n",
    "\n",
    "# Define the output directory for processed files\n",
    "output_dir = base_dir / \"Personal Spend\" / \"02 Individual Datasets\" / \"2024\"\n",
    "# This constructs the full path to the directory where processed output files will be saved.\n",
    "# Example: \"C:/Users/YourUserName/Documents/Finanzas/Personal Spend/02 Individual Datasets/2024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f25916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to construct the input file path for a specific month and year\n",
    "def get_filename_input(month, year, month_text, prefix=\"\", suffix=\"\", extension=\".xlsx\"):\n",
    "    \"\"\"\n",
    "    Constructs a filename for the input file based on the provided parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - month (int): The numeric month (e.g., 1 for January, 12 for December).\n",
    "    - year (int): The year (e.g., 2024).\n",
    "    - month_text (str): The textual representation of the month (e.g., \"January\").\n",
    "    - prefix (str): An optional prefix for the file name (default is an empty string).\n",
    "    - suffix (str): An optional suffix for the file name (default is an empty string).\n",
    "    - extension (str): The file extension (default is \".xlsx\").\n",
    "\n",
    "    Returns:\n",
    "    - str: The constructed file name (e.g., \"01_suffix.xlsx\").\n",
    "    \"\"\"\n",
    "    return str(month).zfill(2) + suffix + extension\n",
    "\n",
    "\n",
    "# Function to construct the output file path for a specific month and year\n",
    "def get_filename_output(month, year, month_text, prefix=\"\", suffix=\"\", extension=\".csv\"):\n",
    "    \"\"\"\n",
    "    Constructs a filename for the output file based on the provided parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - month (int): The numeric month (e.g., 1 for January, 12 for December).\n",
    "    - year (int): The year (e.g., 2024).\n",
    "    - month_text (str): The textual representation of the month (e.g., \"January\").\n",
    "    - prefix (str): An optional prefix for the file name (default is an empty string).\n",
    "    - suffix (str): An optional suffix for the file name (default is an empty string).\n",
    "    - extension (str): The file extension (default is \".csv\").\n",
    "\n",
    "    Returns:\n",
    "    - str: The constructed file name with the format \"month_text/month_textMM.csv\".\n",
    "    \"\"\"\n",
    "    return f\"{month_text}/{prefix}{month_text}{str(month).zfill(2)}{extension}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40bd1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file paths for the current month's input and output files\n",
    "\n",
    "# Construct the full input file path\n",
    "input_file = input_dir / get_filename_input(\n",
    "    current_month,\n",
    "    current_year,\n",
    "    current_month_text,\n",
    "    prefix=\"\",\n",
    "    suffix=\"-actividad\"\n",
    ")\n",
    "\n",
    "# Construct the full output file path\n",
    "output_file = output_dir / get_filename_output(\n",
    "    current_month,\n",
    "    current_year,\n",
    "    current_month_text,\n",
    "    prefix=\"df-amex-tdc-\",\n",
    "    suffix=\"\"\n",
    ")\n",
    "\n",
    "# Print the constructed file paths\n",
    "print(\"Input file path: \", input_file)\n",
    "print(\"Output file path: \", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b222514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import monthly data from the specified Excel file\n",
    "\n",
    "try:\n",
    "    # Attempt to read the Excel file, skipping the first 6 rows\n",
    "    df_amex = pd.read_excel(input_file, skiprows=6)\n",
    "    print(\"Data imported successfully!\")\n",
    "except FileNotFoundError:\n",
    "    # Handle the case where the input file is not found\n",
    "    print(f\"File not found: {input_file}\")\n",
    "except Exception as e:\n",
    "    # Handle any other exceptions that may occur during file import\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997f6620",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "Extract contains\n",
    "- **Fecha**: Date in DD Month YYYY format\n",
    "- **Fecha de Compra**: Purchase Date in DD Month YYYY format\n",
    "- **Descripción**: Description that contains the entity.\n",
    "- **Importe**: Charged amount in local currency (MXN)\n",
    "- **Monto en moneda extranjera:** Charged amount in foreign currency\n",
    "- **Tipo de cambio:** Rate\n",
    "- **Información Adicional:** RFC (tax) information\n",
    "- **Aparece en su Estado de Cuenta como:** Name used in the bank statement\n",
    "- **Dirección:**Address\n",
    "- **Población/Provincia:** Region\n",
    "- **Código Postal:** Postal Code\n",
    "- **País:** Country\n",
    "- **Referencia:** Reference ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c4513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first few rows of the imported data\n",
    "try:\n",
    "    # Display the first five rows of the DataFrame\n",
    "    print(\"Preview of the imported data:\")\n",
    "    display(df_amex.head())\n",
    "except NameError:\n",
    "    # Handle the case where the DataFrame does not exist\n",
    "    print(\"The data has not been successfully loaded into a DataFrame. Please check the file import process.\")\n",
    "except Exception as e:\n",
    "    # Handle any other unexpected errors\n",
    "    print(f\"An error occurred during data visualization: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a843fe1",
   "metadata": {},
   "source": [
    "### Basic Data Cleansing\n",
    "\n",
    "This section focuses on preparing the financial dataset for analysis by removing irrelevant rows, handling missing values, transforming data types, and splitting columns for better structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d823ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Fecha' column to datetime format (YYYY-MM-DD)\n",
    "df_amex['Fecha'] = pd.to_datetime(df_amex['Fecha'], format='%d %b %Y')\n",
    "# Converts the 'Fecha' column to a datetime object using the format \"%d %b %Y\".\n",
    "# Example: \"01 Jan 2024\" becomes \"2024-01-01\".\n",
    "\n",
    "\n",
    "# Convert the 'Importe' column to float\n",
    "df_amex['Importe'] = df_amex['Importe'].astype(float)\n",
    "# Ensures that the 'Importe' column contains numeric values in float format.\n",
    "# This is important for numerical calculations or aggregations.\n",
    "\n",
    "# Split the 'Monto en moneda extranjera' to obtain the currency and amount\n",
    "df_amex[['Amount_Foreign_Currency', 'Foreign_Currency']] = df_amex['Monto en moneda extranjera'].str.extract(r'(\\d+(?:\\.\\d+)?)\\s*([A-Za-z]+)')\n",
    "\n",
    "df_amex['Amount_Foreign_Currency'] = df_amex['Amount_Foreign_Currency'].astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b07796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(input_str):\n",
    "    \"\"\"\n",
    "    Removes accents from a given string.\n",
    "\n",
    "    Parameters:\n",
    "    - input_str (str): The input string from which accents need to be removed.\n",
    "\n",
    "    Returns:\n",
    "    - str: The input string without accent marks.\n",
    "    \"\"\"\n",
    "    # Step 1: Normalize the string to decompose accented characters into base characters and diacritical marks\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    \n",
    "    # Step 2: Remove diacritical marks by filtering out combining characters\n",
    "    return ''.join([c for c in nfkd_form if not unicodedata.combining(c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0c50363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to clean the 'Descripción' column and create a new 'Desc_cleaned' column\n",
    "df_amex['Desc_cleaned'] = df_amex['Descripción'].apply(remove_accents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca47df",
   "metadata": {},
   "source": [
    "### Entity Harmonization\n",
    "\n",
    "This section focuses on standardizing supplier names within the dataset by applying fuzzy matching techniques against a predefined mapping. The goal is to ensure consistency in supplier names for easier analysis and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2525a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonize_supplier(supplier_name, entity_mapping):\n",
    "    \"\"\"\n",
    "    Harmonizes a supplier name using fuzzy matching against a predefined mapping.\n",
    "\n",
    "    Parameters:\n",
    "    - supplier_name (str): The name of the supplier to be harmonized.\n",
    "    - entity_mapping (dict): A dictionary where keys are potential supplier names \n",
    "                             and values are their harmonized names.\n",
    "\n",
    "    Returns:\n",
    "    - str: The harmonized supplier name if a close match is found; \n",
    "           otherwise, returns the original supplier name.\n",
    "    \"\"\"\n",
    "    # Use fuzzy matching to find potential matches for the supplier name\n",
    "    matches = get_close_matches(supplier_name, entity_mapping.keys(), n=1, cutoff=0.8)\n",
    "    \n",
    "    # Return the harmonized name if a close match is found\n",
    "    if matches:\n",
    "        return entity_mapping[matches[0]]\n",
    "    \n",
    "    # Return the original name if no close match is found\n",
    "    return supplier_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a01a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the list of suppliers from the 'DESCRIPCION_ENTIDAD' column\n",
    "suppliers_to_harmonize = df_amex['Desc_cleaned'].tolist()\n",
    "\n",
    "# Step 1: Harmonize the supplier names using the mapping\n",
    "harmonized_suppliers = [harmonize_supplier(supplier, entity_mapping) for supplier in suppliers_to_harmonize]\n",
    "\n",
    "# Step 2: Add the harmonized supplier names back to the DataFrame as a new column\n",
    "df_amex['Harmonized_Supplier'] = harmonized_suppliers\n",
    "\n",
    "\n",
    "# Step 3: Display both 'DESCRIPCION_ENTIDAD' and 'Harmonized_Supplier' for comparison\n",
    "print(\"Comparison of original and harmonized supplier names:\")\n",
    "display(df_amex[['Desc_cleaned', 'Harmonized_Supplier']])\n",
    "\n",
    "# Step 4: Review the output and if necessary, update the entity_mapping.py dictionary and re-run the script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc745e",
   "metadata": {},
   "source": [
    "### Transaction Type Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "982db059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign transaction type descriptions based on the 'ABONO' column\n",
    "df_amex['TXT_TRANSACTION_TYPE'] = np.where(\n",
    "    df_amex['Importe'] > 0,  # Condition: If the value in 'ABONO' is not zero\n",
    "    \"Expense\",             # Value to assign if the condition is True\n",
    "    \"Deposit\"                 # Value to assign if the condition is False\n",
    ")\n",
    "# Transactions with higher than zero values in the 'Importe' column are categorized as \"Expense\".\n",
    "# All other transactions are categorized as \"Deposit\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7141c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb6b9435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonize_transaction_subtype(transaction_desc, transaction_subtype_mapping):\n",
    "    \"\"\"\n",
    "    Harmonizes the transaction subtype description using fuzzy matching against a predefined mapping.\n",
    "\n",
    "    Parameters:\n",
    "    - transaction_desc (str): The name of the transaction descriptions to be harmonized.\n",
    "    - entity_mapping (dict): A dictionary where keys are potential transaction descriptions \n",
    "                             and values are their harmonized names.\n",
    "\n",
    "    Returns:\n",
    "    - str: The harmonized transaction subtype if a close match is found; \n",
    "           otherwise, returns the original transaction description.\n",
    "    \"\"\"\n",
    "    # Use fuzzy matching to find potential matches for the transaction subtype\n",
    "    matches = get_close_matches(transaction_desc, transaction_subtype_mapping.keys(), n=1, cutoff=0.8)\n",
    "    \n",
    "    # Return the harmonized name if a close match is found\n",
    "    if matches:\n",
    "        return transaction_subtype_mapping[matches[0]]\n",
    "    \n",
    "    # Return the original name if no close match is found\n",
    "    return transaction_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1644a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the list of transaction descriptions from the 'Desc_cleaned' column\n",
    "transactions_to_harmonize = df_amex['Desc_cleaned'].tolist()\n",
    "\n",
    "# Step 1: Harmonize the transaction descriptions using the mapping\n",
    "harmonized_transactions = [harmonize_transaction_subtype(transaction, transaction_subtype_mapping) for transaction in transactions_to_harmonize]\n",
    "\n",
    "# Step 2: Add the harmonized supplier names back to the DataFrame as a new column\n",
    "df_amex['Harmonized_Transaction_Subtype'] = harmonized_transactions\n",
    "\n",
    "# Step 3: Display both 'DESCRIPCION_DETALLE' and 'harmonized_transactions' for comparison\n",
    "print(\"Comparison of original and harmonized transaction:\")\n",
    "display(df_amex[['Desc_cleaned', 'Harmonized_Transaction_Subtype']])\n",
    "\n",
    "# Step 4: Review the output and if necessary, update the transaction_subtype_mapping.p dictionary and re-run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed36f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amex.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2475c3",
   "metadata": {},
   "source": [
    "### Data Cleansing and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cbdbbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column that identifies the source system\n",
    "df_amex['KEY_SYSTEM'] = 'AMEX'\n",
    "\n",
    "# Create a new column that identifies the source account\n",
    "df_amex['KEY_ACCOUNT'] = 'AMEXTDCORO'\n",
    "df_amex['TXT_ACCOUNT'] = 'AMEX TDC'\n",
    "\n",
    "# Create a new column that combines CARGO and ABONO columns\n",
    "df_amex['NUM_AMT_NET_REPORTING'] = df_amex['Importe'] * -1\n",
    "\n",
    "# Create columns that contains the currency-related information\n",
    "df_amex['NUM_AMT_DOCUMENT'] = np.where(\n",
    "    pd.notna(df_amex['Amount_Foreign_Currency']),  # Check for non-NaN values\n",
    "    df_amex['Amount_Foreign_Currency'],  # Use the amount if not NaN\n",
    "    df_amex['Importe'] * -1  # Use this for NaN rows\n",
    ")\n",
    "\n",
    "df_amex['KEY_CURRENCY_DOCUMENT'] = np.where(\n",
    "    pd.notna(df_amex['Amount_Foreign_Currency']),  # Check for non-NaN values\n",
    "    'Foreign_currency',  # Use 'Foreign_currency' if not NaN\n",
    "    'MXN'  # Use 'MXN' for NaN rows\n",
    ")\n",
    "\n",
    "df_amex['KEY_RATE'] = df_amex['Tipo de Cambio']\n",
    "\n",
    "# Create new columns with time information\n",
    "df_amex['KEY_MONTH'] = df_amex['Fecha'].dt.month\n",
    "df_amex['KEY_YEAR'] = df_amex['Fecha'].dt.year\n",
    "\n",
    "# Create a new column that contains the flag indicating whether the transaction is debit or credit\n",
    "df_amex['FLG_DEBIT_CREDIT'] = np.where(df_amex['Importe'] > 0, 'C', 'D')\n",
    "\n",
    "# Create new columns that identify the grouping operation (project, vacation, etc.). These will be blank and filled in later.\n",
    "df_amex['KEY_OPERATION'] = ''\n",
    "df_amex['TXT_OPERATION'] = ''\n",
    "\n",
    "# Create new columns relevant for credit card transactions and payments in installments. Not relevant for this account.\n",
    "\n",
    "# Logic to create DUE_DATE column\n",
    "def calculate_due_date(row):\n",
    "    if row['Importe'] >= 2400 and row['Amount_Foreign_Currency'] != 'USD':\n",
    "        due_date = row['Fecha'] + relativedelta(months=2)\n",
    "        return due_date.replace(day=3)\n",
    "    else:\n",
    "        return None  # Or any default value\n",
    "\n",
    "df_amex['DUE_DATE'] = df_amex.apply(calculate_due_date, axis=1)\n",
    "\n",
    "df_amex['KEY_PAYMENT_TERM'] = 'MSI3'\n",
    "df_amex['TXT_PAYMENT_TERM'] = '3 MSI'\n",
    "\n",
    "# Logic to calculate the amount due\n",
    "def calculate_due_amount(row):\n",
    "    if row['Desc_cleaned'] == \"MESES EN AUTOMATICO NACIONAL\":\n",
    "        return row['Importe'] * 3 * -1\n",
    "    else:\n",
    "        return None  # Or any default value\n",
    "\n",
    "\n",
    "df_amex['NUM_AMT_DUE'] = df_amex.apply(calculate_due_amount, axis=1).fillna(0)\n",
    "\n",
    "df_amex['KEY_ID_DUE'] = ''\n",
    "df_amex['TXT_ENTITY_DUE'] = ''\n",
    "df_amex['TXT_DESC_DUE'] = ''\n",
    "\n",
    "# Create new columns with the country information. This account does not provide the country information.\n",
    "\n",
    "# Mapping of country names to country codes\n",
    "country_mapping = {\n",
    "    \"MEXICO\": \"MX\",\n",
    "    \"UNITED STATES\": \"US\",\n",
    "    \"GBP\": \"UK\",\n",
    "    \"EUR\": \"EU\"\n",
    "}\n",
    "\n",
    "df_amex['KEY_COUNTRY'] = df_amex['País'].map(country_mapping).fillna(\"MX\")\n",
    "\n",
    "# Mapping of country names to country names\n",
    "name_country_mapping = {\n",
    "    \"MEXICO\": \"Mexico\",\n",
    "    \"UNITED STATES\": \"United States\",\n",
    "    \"GBP\": \"United Kingdom\"\n",
    "}\n",
    "\n",
    "df_amex['TXT_COUNTRY'] = df_amex['País'].map(name_country_mapping).fillna(\"Mexico\")\n",
    "\n",
    "# Create a new column with the purchase document number, blank initially.\n",
    "df_amex['KEY_PURCH_DOC_NO'] = ''\n",
    "\n",
    "# Create a new column with internal flag\n",
    "df_amex['FLG_INTERNAL'] = np.where(df_amex['Harmonized_Supplier'] == 'BBVA Ricardo Perez Castillo', 'Y', 'N')\n",
    "\n",
    "# Create a new column with flag indicating whether the transaction was canceled\n",
    "df_amex['FLG_CANCEL'] = ''\n",
    "\n",
    "# Create a new column with flag indicating whether the transaction was refunded\n",
    "df_amex['FLG_REFUND'] = (\n",
    "    (df_amex['Importe'] < 0) & \n",
    "    (~df_amex['Desc_cleaned'].isin([\"MONTO A DIFERIR MESES EN AUTOMATICO\", \"GRACIAS POR SU PAGO CON CARGO A BBVA\"]))\n",
    ")\n",
    "\n",
    "# Create new columns that will be filled later with the master tables\n",
    "df_amex['KEY_ENTITY'] = ''\n",
    "df_amex['KEY_TRANSACTION_TYPE'] = ''\n",
    "df_amex['KEY_TRANSACTION_SUBTYPE'] = ''\n",
    "\n",
    "# Rename the columns to match the standard naming convention\n",
    "df_amex.rename(columns={\n",
    "    'Fecha': 'KEY_DATE',\n",
    "    'Desc_cleaned': 'TXT_DESC',\n",
    "    'Harmonized_Supplier': 'TXT_ENTITY',\n",
    "    'Harmonized_Transaction_Subtype': 'TXT_TRANSACTION_SUBTYPE',\n",
    "    'Referencia': 'KEY_ID'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b25fcd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the columns based on the standard order\n",
    "df_amex = df_amex[[\n",
    "    'KEY_ID', 'KEY_SYSTEM', 'KEY_ACCOUNT', 'TXT_ACCOUNT', 'KEY_DATE', 'KEY_MONTH', 'KEY_YEAR',\n",
    "    'KEY_ENTITY', 'TXT_ENTITY', 'KEY_TRANSACTION_TYPE', 'TXT_TRANSACTION_TYPE', 'KEY_TRANSACTION_SUBTYPE',\n",
    "    'TXT_TRANSACTION_SUBTYPE', 'TXT_DESC', 'NUM_AMT_NET_REPORTING', 'NUM_AMT_DOCUMENT', 'KEY_CURRENCY_DOCUMENT',\n",
    "    'KEY_RATE', 'FLG_DEBIT_CREDIT', 'KEY_OPERATION', 'TXT_OPERATION', 'DUE_DATE', 'KEY_PAYMENT_TERM',\n",
    "    'TXT_PAYMENT_TERM', 'NUM_AMT_DUE', 'KEY_ID_DUE', 'TXT_ENTITY_DUE', 'TXT_DESC_DUE', 'KEY_COUNTRY',\n",
    "    'TXT_COUNTRY', 'KEY_PURCH_DOC_NO', 'FLG_INTERNAL', 'FLG_CANCEL', 'FLG_REFUND'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618fb0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization of the processed data\n",
    "df_amex.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41600a5c-60dc-400e-b64e-e4dfe52dd4dd",
   "metadata": {},
   "source": [
    "## 05. Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f0c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the cleaned and processed DataFrame to a CSV file\n",
    "\n",
    "try:\n",
    "    # Export DataFrame to the specified output file\n",
    "    df_bbva.to_csv(output_file, index=False)\n",
    "    print(f\"File successfully exported to: {output_file}\")\n",
    "except FileNotFoundError:\n",
    "    # Handle the case where the output directory does not exist\n",
    "    print(f\"Output path not found: {output_file}\")\n",
    "except PermissionError:\n",
    "    # Handle permission issues when writing to the file\n",
    "    print(f\"Permission denied while trying to write to: {output_file}\")\n",
    "except Exception as e:\n",
    "    # Handle any other unforeseen errors during the export\n",
    "    print(f\"An unexpected error occurred while writing the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125a8234-00d4-4b07-9856-eddc8c10cb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personalspend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
